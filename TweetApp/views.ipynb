{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import operator\n",
    "import re\n",
    "import collections\n",
    "import hashlib\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import colorlover as cl\n",
    "from plotly.offline.offline import _plot_html\n",
    "\n",
    "\n",
    "from googletrans import Translator\n",
    "from collections import Counter\n",
    "import translate\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import vincent\n",
    "from bokeh.embed import components\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.plotting import figure\n",
    "from django.http import HttpResponse\n",
    "from django.http import JsonResponse\n",
    "from django.shortcuts import render\n",
    "\n",
    "from django.template import loader\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from TweetApp import models\n",
    "from .forms import SignupForm\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from string import punctuation\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from aylienapiclient import textapi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "cache_english_stopwords = stopwords.words('english')  # type: object\n",
    "cache_spanish_stopwords = stopwords.words('spanish')  # type: object\n",
    "cache_portuguese_stopwords = stopwords.words('portuguese')  # type: object\n",
    "\n",
    "client = textapi.Client('fa3ed458', 'f7904358a4d58ce3fa0d9415801bb481')\n",
    "\n",
    "# punctuation = list(string.punctuation)\n",
    "# stop = stopwords.words('spanish') + stopwords.words('english') + punctuation + ['rt', 'via', 'RT', '...', 'sa']\n",
    "# com = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# tokens_re = re.compile(r'(' + '|'.join(regex_str) + ')', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "\n",
    "# emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "'''def tokenize(s):\n",
    "    return tokens_re.findall(s)'''\n",
    "\n",
    "USER = \"\"\n",
    "IDIOMA = \"\"\n",
    "ESTACION = \"\"\n",
    "HASHTAGS = []\n",
    "\n",
    "py.sign_in('d.chichell', 'MmTDIsdVOIjwUMiSOCvM')\n",
    "\n",
    "def signupform(request):\n",
    "    if request.method == 'POST':\n",
    "        form = SignupForm(request.POST)\n",
    "        if form.is_valid():\n",
    "            return render(request, 'result.html', {\n",
    "                'name': form.cleaned_data['name'],\n",
    "                'email': form.cleaned_data['email'],\n",
    "            })\n",
    "\n",
    "    else:\n",
    "        form = SignupForm()\n",
    "\n",
    "    return render(request, 'signupform.html', {'form': form})\n",
    "\n",
    "###############################################\n",
    "'''Vistas pertenecientes a login del usuario'''\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "def login(request):\n",
    "    if request.method == 'POST':\n",
    "        template = loader.get_template('index.html')\n",
    "        context = {}\n",
    "        email = request.POST['email']\n",
    "        username = request.POST['username']\n",
    "        password = request.POST['password']\n",
    "\n",
    "        email = hashlib.sha256(email.encode('utf-8')).hexdigest()\n",
    "        username = hashlib.sha256(username.encode('utf-8')).hexdigest()\n",
    "        password = hashlib.sha256(password.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "        models.insertUser(email, username, password)\n",
    "\n",
    "        return HttpResponse(template.render(context, request))\n",
    "    else:\n",
    "        template = loader.get_template('index.html')\n",
    "        context = {}\n",
    "        return HttpResponse(template.render(context, request))\n",
    "\n",
    "\n",
    "def result(request):\n",
    "    template = loader.get_template('result.html')\n",
    "    #username = request.POST['username']\n",
    "    #password = request.POST['password']\n",
    "    #check = models.checkPassUser(username, password)\n",
    "\n",
    "    context = {'username': USER}\n",
    "    global IDIOMA\n",
    "    IDIOMA = request.POST['drop1']\n",
    "    global ESTACION\n",
    "    ESTACION = request.POST['drop2']\n",
    "    #global HASHTAGS\n",
    "    #HASHTAGS = request.POST['drop2']\n",
    "    return HttpResponse(template.render(context, request))\n",
    "\n",
    "def config(request):\n",
    "    if request.method == \"POST\":\n",
    "        template = loader.get_template('config.html')\n",
    "        username = request.POST['username']\n",
    "        password = request.POST['password']\n",
    "        check = models.checkPassUser(username, password)\n",
    "        if check:\n",
    "            global USER\n",
    "            USER = request.POST['username']\n",
    "            context = {'username': username}\n",
    "            return HttpResponse(template.render(context, request))\n",
    "        else:\n",
    "            template = loader.get_template('index.html')\n",
    "            context = {'error': True}\n",
    "            return HttpResponse(template.render(context, request))\n",
    "    else:\n",
    "        template = loader.get_template('config.html')\n",
    "        context = {'username':USER}\n",
    "        return HttpResponse(template.render(context, request))\n",
    "\n",
    "def mostrarRegistro(request):\n",
    "    template = loader.get_template('registro.html')  # type: object\n",
    "    context = {}\n",
    "    return HttpResponse(template.render(context, request))\n",
    "\n",
    "\n",
    "###############################################\n",
    "\n",
    "def getJson(request):\n",
    "    json_data = open(\"static/area.json\").read()\n",
    "    data = json.loads(json_data)\n",
    "    return JsonResponse(data)\n",
    "\n",
    "\n",
    "def getImage(request):\n",
    "    with open(\"static/sentimientos.png\", \"rb\") as f:\n",
    "        return HttpResponse(f.read(), mimetype=\"image/png\")\n",
    "\n",
    "\n",
    "def index(request):\n",
    "    word = \"hola\"\n",
    "    context = {\n",
    "        'palabra': word,\n",
    "    }\n",
    "    template = loader.get_template('index.html')\n",
    "    return HttpResponse(template.render(context, request))\n",
    "\n",
    "def inicio(request):\n",
    "    return HttpResponse(\"¡¡Bienvenido!! Inicio de la web del sistema sanitario\")\n",
    "\n",
    "\n",
    "##############################NUEVO################################################\n",
    "\n",
    "def wordcloud(text):\n",
    "    tokens = [word for sent in text for word in sent.split()]\n",
    "    text = ' '.join(tokens)\n",
    "    wordcloud = WordCloud(max_font_size=40).generate(text)\n",
    "\n",
    "    wordcloud.to_file('static/wordcloud.jpg')\n",
    "    return ('wordcloud.jpg')\n",
    "\n",
    "\n",
    "def word_frequency(texts):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    freq = np.ravel(X.sum(axis=0))  # sum each columns to get total counts for each word\n",
    "\n",
    "    # get vocabulary keys, sorted by value\n",
    "    vocab = [v[0] for v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]\n",
    "    fdist = dict(zip(vocab, freq))  # return same format as nltk\n",
    "    sorted_fdist = sorted(fdist.items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "\n",
    "    x_data = [x[0] for x in sorted_fdist]\n",
    "    y_data = [x[1] for x in sorted_fdist]\n",
    "\n",
    "    # Set the x_range to the list of categories above\n",
    "    p = figure(x_range=x_data, plot_height=300, plot_width=498, title=\"Palabras más frecuentes\")\n",
    "\n",
    "    # Categorical values can also be used as coordinates\n",
    "    p.vbar(x=x_data, top=y_data, width=0.9)\n",
    "\n",
    "    # Set some properties to make the plot look better\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.y_range.start = 0\n",
    "    p.xaxis.axis_label = \"Palabras\"\n",
    "    p.yaxis.axis_label = \"Nº de apariciones\"\n",
    "\n",
    "    script, div = components(p)  # plot\n",
    "    contexto = {'script': script, 'div': div}\n",
    "\n",
    "    return script, div\n",
    "    # return render(request, 'graph.html', contexto)\n",
    "\n",
    "def user_frequency(texts):\n",
    "    counter = collections.Counter(texts)\n",
    "    sorted_fdist = counter.most_common(4)\n",
    "\n",
    "    x_data = [x[0] for x in sorted_fdist]\n",
    "    y_data = [x[1] for x in sorted_fdist]\n",
    "\n",
    "    p = figure(x_range=x_data, plot_height=300, plot_width=512, title=\"Nº de tweets por regiones (solo usuarios con localización)\")\n",
    "\n",
    "    p.vbar(x=x_data, top=y_data, width=0.9)\n",
    "\n",
    "    p.xgrid.grid_line_color = None\n",
    "    p.y_range.start = 0\n",
    "    p.xaxis.axis_label = \"Usuario\"\n",
    "    p.yaxis.axis_label = \"Nº de Tweets publicados\"\n",
    "\n",
    "    script, div = components(p)  # plot\n",
    "    contexto = {'script': script, 'div': div}\n",
    "\n",
    "    return script, div\n",
    "    # return render(request, 'graph.html', contexto)\n",
    "\n",
    "\n",
    "def lda(texts):\n",
    "    NUM_TOPICS = 10\n",
    "    vectorizer = CountVectorizer(min_df=5, max_df=0.9,\n",
    "                                 stop_words='english', lowercase=True,\n",
    "                                 token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "    data_vectorized = vectorizer.fit_transform(texts)\n",
    "\n",
    "    # Build a Latent Dirichlet Allocation Model\n",
    "    lda_model = LatentDirichletAllocation(n_topics=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "    lda_Z = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "    def print_topics(model, vectorizer, top_n=10):\n",
    "        for idx, topic in enumerate(model.components_):\n",
    "            print(\"Topic %d:\" % (idx))\n",
    "            print([(vectorizer.get_feature_names()[i], topic[i])\n",
    "                   for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "\n",
    "\n",
    "    # plotear las palabras\n",
    "    svd = TruncatedSVD(n_components=2)\n",
    "    words_2d = svd.fit_transform(data_vectorized.T)\n",
    "\n",
    "    df = pd.DataFrame(columns=['x', 'y', 'word'])\n",
    "    df['x'], df['y'], df['word'] = words_2d[:, 0], words_2d[:, 1], vectorizer.get_feature_names()\n",
    "\n",
    "    source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "    labels = LabelSet(x=\"x\", y=\"y\", text=\"word\", y_offset=8,\n",
    "                      text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                      source=source, text_align='center')\n",
    "\n",
    "    plot = figure(plot_height=300, plot_width=498)\n",
    "    plot.circle(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.8)\n",
    "    plot.add_layout(labels)\n",
    "\n",
    "    script, div = components(plot)  # plot\n",
    "    contexto = {'script2': script, 'div2': div}\n",
    "\n",
    "    return script, div\n",
    "    # return render(request, 'graph.html', contexto)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentimental2(texts, idioma):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(texts)\n",
    "    freq = np.ravel(X.sum(axis=0))  # sum each columns to get total counts for each word\n",
    "\n",
    "    # get vocabulary keys, sorted by value\n",
    "    vocab = [v[0] for v in sorted(vectorizer.vocabulary_.items(), key=operator.itemgetter(1))]\n",
    "    fdist = dict(zip(vocab, freq))  # return same format as nltk\n",
    "    sorted_fdist = sorted(fdist.items(), key=operator.itemgetter(1), reverse=True)[:500]\n",
    "    print(sorted_fdist)\n",
    "    diccionarioSentimientos = {}\n",
    "    x = []\n",
    "    y = []\n",
    "    frequency = [x[1] for x in sorted_fdist]\n",
    "    for key, value in sorted_fdist:\n",
    "        if idioma != 'en':\n",
    "            #traducido = Translator().translate(text=key,dest='en')\n",
    "            #traducido = translate.Translator(idioma, 'en', key)\n",
    "            diccionarioSentimientos[key] = analize_sentiment(key)\n",
    "            '''translator = Translator()\n",
    "            traducido = translator.translate(text=key,dest='en',src=idioma)\n",
    "            diccionarioSentimientos[key] = analize_sentiment(str(traducido))'''\n",
    "        else:\n",
    "            diccionarioSentimientos[key] = analize_sentiment(key)\n",
    "        #diccionarioSentimientos = dict(zip(key,value))\n",
    "    print(diccionarioSentimientos)\n",
    "    for key, value in diccionarioSentimientos.items():\n",
    "        x.append(key)\n",
    "        y.append(value)\n",
    "\n",
    "    # Fixing random state for reproducibility\n",
    "    colors = cl.scales['3']['seq']['Blues']\n",
    "    data = {'Palabra': [1, 0, -1],\n",
    "            'Polaridad': colors}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    trace0 = go.Table(\n",
    "        type='table',\n",
    "        header=dict(\n",
    "            values=[\"Palabra\",\"Nº de apariciones\",\"Polaridad [-1, 1]\"],\n",
    "            line=dict(color='black'),\n",
    "            fill=dict(color='white'),\n",
    "            align=['center'],\n",
    "            font=dict(color='black', size=12)\n",
    "        ),\n",
    "        cells=dict(\n",
    "            values=[x,frequency,y],\n",
    "            #line=dict(color=[df.Polaridad]),\n",
    "            #fill=dict(color=[df.Polaridad]),\n",
    "            fill=dict(color=['rgb(245,245,245)','rgb(245,245,245)',  # unique color for the first column\n",
    "                             ['rgba(250,0,0, 0.8)' if val < 0 else ('rgb(245,245,245)' if val == 0  else 'rgba(0,250,0, 0.8)') for val in y]]),\n",
    "                            #a = \"neg\" if b<0 else \"pos\" if b>0 else \"zero\"\n",
    "            align='center',\n",
    "            font=dict(color='black', size=11)\n",
    "        ))\n",
    "    data = [trace0]\n",
    "    '''trace = go.Table(\n",
    "        header=dict(values=['Palabra', 'Polaridad [-1, 1]']),\n",
    "        #cells=dict(values=[[100, 90, 80, 90],                   [95, 85, 75, 95]]))\n",
    "        cells = dict(values=[x,y])\n",
    "    )\n",
    "    data = [trace]'''\n",
    "\n",
    "    '''plt.scatter(x,y)\n",
    "    #plt.Axes.tick_params(labelsize='small')\n",
    "    #plt.axes(labelsize='small')\n",
    "\n",
    "\n",
    "    plt.tick_params(axis='x', labelsize=6)\n",
    "    fig1 = plt.gcf()\n",
    "\n",
    "    fig1.canvas()\n",
    "\n",
    "\n",
    "    fig1.set_size_inches(10, 3, forward=False)\n",
    "    fig1.savefig('static/sentimientos.png')\n",
    "    sentimentalGraph = \"sentimientos.png\"\n",
    "    fig1.clf()\n",
    "    contexto = {'sentimientos': sentimentalGraph}'''\n",
    "\n",
    "    #layout = go.Layout(width=1000, height=330)\n",
    "    fig = go.Figure(data=data)\n",
    "\n",
    "    plotly.offline.plot(fig, filename='static/table.html', auto_open=False)\n",
    "    #sentimentalGraph = \"a-simple-plot.png\"\n",
    "    return 0\n",
    "\n",
    "    ################################################\n",
    "def pieSentimental(texts):\n",
    "    texto = texts\n",
    "    data = pd.DataFrame(data=texto, columns=['Tweets'])\n",
    "\n",
    "    #translator = Translator()\n",
    "    #traducido = translator.translate(text=tweet, dest='en', src='auto')\n",
    "    #data['SA'] = np.array([analize_sentiment(str(translator.translate(text=tweet, dest='en', src='auto'))) for tweet in data['Tweets']])\n",
    "    data['SA'] = np.array([analize_sentiment(tweet) for tweet in data['Tweets']])\n",
    "    pos_tweets = [tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] > 0]\n",
    "    neu_tweets = [tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] == 0]\n",
    "    neg_tweets = [tweet for index, tweet in enumerate(data['Tweets']) if data['SA'][index] < 0]\n",
    "\n",
    "    pos = len(pos_tweets) * 100 / len(data['Tweets'])\n",
    "    neg = len(neu_tweets) * 100 / len(data['Tweets'])\n",
    "    neu = len(neg_tweets) * 100 / len(data['Tweets'])\n",
    "\n",
    "    pos = float(\"{0:.1f}\".format(pos))\n",
    "    neg = float(\"{0:.1f}\".format(neg))\n",
    "    neu = float(\"{0:.1f}\".format(neu))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    colors2 = ['green', 'red', 'grey']\n",
    "    sizes2 = [pos, neg, neu]\n",
    "    labels2 = 'Positivos ' + str(pos)+\"%\", 'Negativos ' + str(neg)+\"%\", 'Neutros ' + str(neu)+\"%\"\n",
    "\n",
    "    ## use matplotlib to plot the chart\n",
    "    plt.pie(\n",
    "        x=sizes2,\n",
    "        shadow=False,\n",
    "        colors=colors2,\n",
    "        #labels=labels2,\n",
    "        startangle=90,\n",
    "    )\n",
    "\n",
    "    plt.title(\"Porcentaje de tweets por estado\")\n",
    "    plt.legend(labels2, loc=\"best\")\n",
    "    fig1 = plt.gcf()\n",
    "    fig1.set_size_inches(4.7, 3, forward=False)\n",
    "    fig1.savefig('static/PieSentimientos.png')\n",
    "    pieSentimental = \"PieSentimientos.png\"\n",
    "    # plt.show()\n",
    "    fig1.clf()\n",
    "\n",
    "    #contexto = {'sentimientos': sentimentalGraph}\n",
    "    return pieSentimental\n",
    "    #return 0\n",
    "\n",
    "def analize_sentiment(tweet):\n",
    "    '''\n",
    "    Utility function to classify the polarity of a tweet\n",
    "    using textblob.\n",
    "    '''\n",
    "    analysis = TextBlob(tweet)\n",
    "    '''if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1'''\n",
    "    return round(analysis.sentiment.polarity, 2)\n",
    "\n",
    "def graphLine(estacion,idioma):\n",
    "    models.dbToCsv(estacion,idioma)\n",
    "    tweets = pd.read_csv('static/tweets.csv')\n",
    "    tweets['created_at'] = pd.to_datetime(pd.Series(tweets['created_at']))\n",
    "\n",
    "    tweets.set_index('created_at', drop=False, inplace=True)\n",
    "\n",
    "    tweets_pm = tweets['created_at'].resample('M').count()\n",
    "\n",
    "    # vincent.core.initialize_notebook()\n",
    "    line = vincent.Line(tweets_pm)\n",
    "    line.axis_titles(x='Meses', y='Nº Tweets')\n",
    "    line.colors(brew='Spectral')\n",
    "    line.to_json('static/area.json')\n",
    "    return 0\n",
    "\n",
    "def graphLineIdioma(lang, estacion):\n",
    "    models.dbToCsv(estacion,lang)\n",
    "    tweets = pd.read_csv('static/tweets.csv')\n",
    "    #if(tweets['lang'] == lang):\n",
    "    #tweets['created_at']['lang'] = pd.to_datetime(pd.Series(tweets['created_at'],tweets['lang']))\n",
    "    lista = []\n",
    "    for creacion, idioma in tweets.itertuples(index=False):\n",
    "        if idioma == lang:\n",
    "            tweets['created_at']= pd.to_datetime(pd.Series(tweets['created_at']))\n",
    "            #tweets['created_at'] = pd.to_datetime(pd.Series(creacion))\n",
    "\n",
    "    tweets.set_index('created_at', drop=False, inplace=True)\n",
    "\n",
    "    tweets_pm = tweets['created_at'].resample('M').count()\n",
    "\n",
    "    # vincent.core.initialize_notebook()\n",
    "    line = vincent.Line(tweets_pm)\n",
    "    line.axis_titles(x='Meses', y='Nº Tweets')\n",
    "    line.colors(brew='Spectral')\n",
    "    line.to_json('static/area.json')\n",
    "    return 0\n",
    "\n",
    "def graficos(request):\n",
    "        a = IDIOMA\n",
    "        b = ESTACION\n",
    "        if (b==\"primavera\"):\n",
    "            tweets = models.searchTweetPrimavera()\n",
    "        elif (b == \"invierno\"):\n",
    "            tweets = models.searchTweetInvierno()\n",
    "        elif (b == \"verano\"):\n",
    "            tweets = models.searchTweetVerano()\n",
    "        elif (b == \"otono\"):\n",
    "            tweets = models.searchTweetOtono()\n",
    "        else:\n",
    "            tweets = models.searchTweet()\n",
    "\n",
    "        idiom = clean = tweets\n",
    "        if (a == \"all\"):\n",
    "\n",
    "            #tweets = models.searchTweet()\n",
    "\n",
    "            texts, image2 = clean_tweet(clean)\n",
    "            #image2 = idiomasFrecuentes(idiom)\n",
    "            textUser = obtain_user(a,b)\n",
    "            script1, div1 = lda(texts)\n",
    "            script2, div2 = word_frequency(texts)\n",
    "\n",
    "            script3, div3 = user_frequency(textUser)\n",
    "\n",
    "\n",
    "            image3 = wordcloud(texts)\n",
    "            image = sentimental2(texts,a)\n",
    "            graphLine(b,\"\")\n",
    "            imagePieSentimental = pieSentimental(texts)\n",
    "\n",
    "            contexto = {'script3': script3,'div3':div3,'idioma': \"Todos\", 'username': USER, 'script1': script1, 'div1': div1, 'script2': script2,\n",
    "                        'div2': div2, 'image2': image2,\n",
    "                        'image': image, 'image3': image3,'estacion':b,'pieSentimental':imagePieSentimental}\n",
    "\n",
    "        elif (a == \"en\"):\n",
    "            #tweets = models.searchTweet()\n",
    "            #texts = clean_tweet_languaje(clean,a)\n",
    "            texts, image2 = clean_tweet_languaje(clean,a)\n",
    "            #textUser = obtain_user_lang(a)\n",
    "            textUser = obtain_user(a, b)\n",
    "            script1, div1 = lda(texts)\n",
    "            script2, div2 = word_frequency(texts)\n",
    "\n",
    "            script3, div3 = user_frequency(textUser)\n",
    "\n",
    "            #image2 = idiomasFrecuentes(idiom)\n",
    "            image3 = wordcloud(texts)\n",
    "            image = sentimental2(texts,a)\n",
    "            #graphLine()\n",
    "            graphLineIdioma(a,b)\n",
    "            imagePieSentimental = pieSentimental(texts)\n",
    "            contexto = {'script3': script3,'div3':div3,'idioma': \"Inglés\", 'username': USER, 'script1': script1, 'div1': div1, 'script2': script2,\n",
    "                        'div2': div2, 'image2': image2,\n",
    "                        'image': image, 'image3': image3,'estacion':b,'pieSentimental':imagePieSentimental}\n",
    "\n",
    "\n",
    "        elif (a == \"es\"):\n",
    "            #tweets = models.searchTweet()\n",
    "            #texts = clean_tweet_languaje(clean, a)\n",
    "            texts, image2 = clean_tweet_languaje(clean,a)\n",
    "            #textUser = obtain_user_lang(a)\n",
    "            textUser = obtain_user(a, b)\n",
    "            script1, div1 = lda(texts)\n",
    "            script2, div2 = word_frequency(texts)\n",
    "            script3, div3 = user_frequency(textUser)\n",
    "            #image2 = idiomasFrecuentes(idiom)\n",
    "            image3 = wordcloud(texts)\n",
    "            image = sentimental2(texts,a)\n",
    "            graphLineIdioma(a,b)\n",
    "            imagePieSentimental = pieSentimental(texts)\n",
    "            contexto = {'script3': script3,'div3':div3,'idioma': \"Español\", 'username': USER, 'script1': script1, 'div1': div1, 'script2': script2,\n",
    "                        'div2': div2, 'image2': image2,\n",
    "                        'image': image, 'image3': image3,'estacion':b, 'pieSentimental':imagePieSentimental}\n",
    "\n",
    "\n",
    "        elif (a == \"pt\"):\n",
    "            #tweets = models.searchTweet()\n",
    "            #texts = clean_tweet_languaje(clean, a)\n",
    "            texts, image2 = clean_tweet_languaje(clean,a)\n",
    "            #textUser = obtain_user_lang(a)\n",
    "            textUser = obtain_user(a, b)\n",
    "            script1, div1 = lda(texts)\n",
    "            script2, div2 = word_frequency(texts)\n",
    "            script3, div3 = user_frequency(textUser)\n",
    "            #image2 = idiomasFrecuentes(idiom)\n",
    "            image3 = wordcloud(texts)\n",
    "            image = sentimental2(texts,a)\n",
    "            graphLineIdioma(a,b)\n",
    "            imagePieSentimental = pieSentimental(texts)\n",
    "            contexto = {'script3': script3,'div3':div3,'idioma': \"Portugués\", 'username': USER, 'script1': script1, 'div1': div1, 'script2': script2,\n",
    "                        'div2': div2, 'image2': image2,\n",
    "                        'image': image, 'image3': image3,'estacion':b, 'pieSentimental':imagePieSentimental}\n",
    "\n",
    "        return render(request, 'graph.html', contexto)\n",
    "\n",
    "def idiomasFrecuentes(file):\n",
    "\n",
    "    data = file['lang'].tolist()\n",
    "\n",
    "    es = 0\n",
    "    pt = 0\n",
    "    en = 0\n",
    "    others = 0\n",
    "    for x in data:\n",
    "        if x == 'es':\n",
    "            es = es + 1\n",
    "        elif x == 'en':\n",
    "            en = en + 1\n",
    "        elif x == 'pt':\n",
    "            pt = pt + 1\n",
    "        else:\n",
    "            others = others + 1\n",
    "\n",
    "    colors2 = ['blue', 'red', 'green', 'grey']\n",
    "    sizes2 = [en, es, pt, others]\n",
    "    labels2 = 'Inglés ' + str(en), 'Español ' + str(es), 'Portugués ' + str(pt), 'Otros ' + str(others)\n",
    "\n",
    "    ## use matplotlib to plot the chart\n",
    "    plt.pie(\n",
    "        x=sizes2,\n",
    "        shadow=False,\n",
    "        colors=colors2,\n",
    "        #labels=labels2,\n",
    "        startangle=90\n",
    "    )\n",
    "\n",
    "    plt.title(\"Nº de tweets por idioma\")\n",
    "    # sentimentalGraph = figure.savefig('static/sentimientos.png', bbox_inches='tight')\n",
    "    plt.legend(labels2, loc=\"best\")\n",
    "    fig2 = plt.gcf()\n",
    "    fig2.set_size_inches(4.7, 3, forward=True)\n",
    "    fig2.savefig('static/idiomas.png')\n",
    "    idioma = \"idiomas.png\"\n",
    "    fig2.clf()\n",
    "    plt.close(fig2)\n",
    "\n",
    "    return (idioma)\n",
    "\n",
    "\n",
    "def clean_tweet(file):\n",
    "    #dataFrame = pd.read_json(file, orient='columns') ORIGINAL\n",
    "    #data = dataFrame['text'].tolist() ORIGINAl\n",
    "    df = pd.DataFrame(list(file))\n",
    "\n",
    "    data = df['text'].tolist()\n",
    "    lista = []\n",
    "    for tweet in data:\n",
    "        lista.append(text_clean(tweet))\n",
    "\n",
    "\n",
    "    return lista, idiomasFrecuentes(df)\n",
    "\n",
    "\n",
    "def obtain_user(a,b):\n",
    "    data = models.searchUbicacion(a,b)\n",
    "    df = pd.DataFrame(list(data))\n",
    "    lista = []\n",
    "    for index, row in df.iterrows():\n",
    "        if row['user']['location'] != '':\n",
    "            lista.append(row['user']['location'])\n",
    "\n",
    "\n",
    "    return lista\n",
    "\n",
    "def obtain_user_lang(lang):\n",
    "    file = 'tweets/PruebaSanidad_1000.json'\n",
    "    with open(file) as data_file:\n",
    "        data = json.load(data_file)\n",
    "        list = []\n",
    "        for v in data:\n",
    "            if v['lang'] == lang and v['user']['location']!='':\n",
    "                list.append(v['user']['location'])\n",
    "    print(list)\n",
    "    return list\n",
    "\n",
    "def clean_tweet_languaje(file, lang):\n",
    "    # Load the first sheet of the JSON file into a data frame\n",
    "    #df = pd.read_json(file, orient='columns')\n",
    "    df = pd.DataFrame(list(file))\n",
    "    if (lang == 'en'):\n",
    "        columna = df[(df['lang'] == 'en')]\n",
    "    elif (lang == 'es'):\n",
    "        columna = df[(df['lang'] == 'es')]\n",
    "    else:\n",
    "        columna = df[(df['lang'] == 'pt')]\n",
    "\n",
    "    data = columna.text.tolist()\n",
    "\n",
    "    l = []\n",
    "    for tweet in data:\n",
    "        s = text_clean(tweet)\n",
    "\n",
    "        l.append(s)\n",
    "\n",
    "    return (l), idiomasFrecuentes(df)\n",
    "\n",
    "\n",
    "def text_clean(tweet):\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    tweet_no_special_entities = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    # Remove tickers\n",
    "    tweet_no_tickers = re.sub(r'\\$\\w*', '', tweet_no_special_entities)\n",
    "    # Remove hyperlinks\n",
    "    tweet_no_hyperlinks = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet_no_tickers)\n",
    "    # Remove hashtags\n",
    "    tweet_no_hashtags = re.sub(r'#\\w*', '', tweet_no_hyperlinks)\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    tweet_no_punctuation = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet_no_hashtags)\n",
    "    # Remove https\n",
    "    tweet_no_https = re.sub(r'http', '', tweet_no_punctuation)\n",
    "    # Remove words with 2 or fewer letters\n",
    "    tweet_no_small_words = re.sub(r'\\b\\w{1,2}\\b', '', tweet_no_https)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    tweet_no_whitespace = re.sub(r'\\s\\s+', ' ', tweet_no_small_words)\n",
    "    tweet_no_whitespace = tweet_no_whitespace.lstrip(' ')  # Remove single space remaining at the front of the tweet.\n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    tweet_no_emojis = ''.join(c for c in tweet_no_whitespace if\n",
    "                              c <= '\\uFFFF')  # Apart from emojis (plane 1), this also removes historic scripts and mathematical alphanumerics (also plane 1), ideographs (plane 2) and more.\n",
    "    # Tokenize: Change to lowercase, reduce length and remove handles\n",
    "    tknzr = TweetTokenizer(preserve_case=False, reduce_len=True,\n",
    "                           strip_handles=True)  # reduce_len changes, for example, waaaaaayyyy to waaayyy.\n",
    "    tw_list = tknzr.tokenize(tweet_no_emojis)\n",
    "    # Remove stopwords\n",
    "    list_no_stopwords = [i for i in tw_list if i not in cache_english_stopwords]\n",
    "    list_no_stopwords = [i for i in list_no_stopwords if i not in cache_spanish_stopwords]\n",
    "    list_no_stopwords = [i for i in list_no_stopwords if i not in cache_portuguese_stopwords]\n",
    "    # Final filtered tweet\n",
    "    tweet_filtered = ' '.join(list_no_stopwords)\n",
    "    return (tweet_filtered)\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
